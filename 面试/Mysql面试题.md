![Mysql执行过程与BufferPool缓存机制](https://cdn.wuzx.cool/Mysql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E4%B8%8EBufferPool%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6.png)

写日志相当于顺序IO, 随机IO性能相当的差

# 1、描述一下数据库事务隔离级别

## 1.1 数据库的四大特性

> + `原子性(Atomicity)`: 原子性是指事务是一个不可分割的工作单位,事务中的操作要么全部成功,要么全部失败 undo log
> + `一致性(Consistency)`: 事务必须使数据库从一个一致性状态变换到另外一个一致性状态 最核心和最本质
> + `隔离性(Isolation)`:事务的隔离性是多个用户并发访问数据库时,数据库为每一个用户开启的事务,不能被其他事务的操作数据所干 扰,多个并发事务之间要相互隔离 mvcc(多版本并发控制)
> + `持久性(Durability)`:持久性是指一个事务一旦被提交,它对数据库中数据的改变就是永久性的,接下来即使数据库发生故障也不应该 对其有任何影响 redo log 

​		首先我们数据库有四大特性，ACID ,隔离性就是其中一个特性，然后数据为了提供并发性能。（MVCC）数据库的事务隔离级别有四种，分别是`读未提交`、`读已提交`、`可重复读`、`序列化`，不同的隔离级别下会产生脏读、幻读、不可重复读等相关问题，因此在选择隔离级别的时候要根据应用场景来决定，使用合适的隔离级别。

+ 脏读: 当有2个以上事务进行操作的时候，无论第二个事务是否有提交动作，都会读取到对应的数据，没有任何隔离性
+ 不可重复:   当一个事务在执行过程中，数据被另外一个事务修改了，造成本次事务前后读取的信息不一样
+ 幻读： 当事务A读取某一个范围的数据的时候，事务B插入了一行，事务A再次读取这个范围数据，会产生幻读

| 隔离级别          | 脏读 | 不可重复  读 | 幻读 |
| ----------------- | ---- | ------------ | ---- |
| READ- UNCOMMITTED | √    | √            | √    |
| READ-COMMITTED    | ×    | √            | √    |
| REPEATABLE- READ  | ×    | ×            | √    |
| SERIALIZABLE      | ×    | ×            | ×    |

SQL 标准定义了四个隔离级别：

- READ-UNCOMMITTED(读取未提交)： 事务的修改，即使没有提交，对其他事务也都是可见的。事务能够读取未提交的数据，这种情况称为脏读。
- READ-COMMITTED(读取已提交)： 事务读取已提交的数据，大多数数据库的默认隔离级别。当一个事务在执行过程中，数据被另外一个事务修改，造成本次事务前后读取的信息不一样，这种情况称为不可重复读。
- REPEATABLE-READ(可重复读)： 这个级别是MySQL的默认隔离级别，它解决了脏读的问题，同时也保证了同一个事务多次读取同样的记录是一致的，但这个级别还是会出现幻读的情况。幻读是指当一个事务A读取某一个范围的数据时，另一个事务B在这个范围插入行，A事务再次读取这个范围的数据时，会产生幻读
- SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。

事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。

因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ（可重读）**并不会有任何性能损失。

# 2. mvcc 底层原理

> 见MVCC多版本并发控制文档

# 3、mysql锁机制

## 3.1 锁分类

+ 从性能上分为乐观锁(用版本对比来实现)和悲观锁

+ 从数据库操作的类型分，读锁和写锁

  + 读锁(S锁(shard) 也叫做共享锁):针对同一份数据，多个读操作可以同时进行而不会影响
  + 写锁(X锁(eXclusive 也叫做排它锁)): 当前写操作没有完成前，他会阻断其他写锁和读锁

+ 从对数据操作的粒度分，分为表锁和行锁

### 表锁：

   > 每次操作锁住整张表。开销小，加锁快;不会出现死锁;锁定粒度大，发生锁冲突的概率最高，并发度最低;
    > 一般用在整表数据迁移的场景。
    >
    > 手动增加表锁
    >
    > ``` shell
    >lock table tableName read/write;
    > ```
    > 
    > 查询加锁的表
    >
    > ``` shell
    >show open tables;
    > ```
    > 
    > 删除表锁
    >
    > ``` shell
    >unlock tables;
    > ```

### 行锁

> 每次操作锁住一行数据。开销大，加锁慢;会出现死锁;锁定粒度最小，发生锁冲突的概率最低，并发度最 高。
>
> MyISAM在执行查询语句SELECT前，会自动给涉及的所有表加读锁,在执行update、insert、delete操作会自 动给涉及的表加写锁。 InnoDB在执行查询语句SELECT时(非串行隔离级别)，不会加锁。但是update、insert、delete操作会加行 锁。
>
> 简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。

## 间隙锁(Gap Lock)

> 间隙锁，锁的就是两个值之间的空隙

![image-20220731232401729](https://cdn.wuzx.cool/image-20220731232401729.png)

+ 那么间隙就有 id 为 (3,10)，(10,20)，(20,正无穷) 这三个区间，
+ 在Session_1下面执行 update account set name = 'zhuge' where id > 8 and id <18;
+ 则其他Session没 法在这个范围所包含的所有行记录(包括间隙行记录)以及行记录所在的间隙里插入或修改任何数据，即id在 (3,20]区间都无法修改数据，注意最后那个20也是包含在内的。

间隙锁是在可重复读隔离级别下才会生效

## 临键锁(Next-key Locks)

Next-Key Locks是`行锁`与`间隙锁`的组合。像上面那个例子里的这个(3,20]的整个区间可以叫做临键锁。

##  无索引行锁会升级为表锁

> 锁主要是加在索引上，如果对非索引字段更新，行锁可能会变表锁
>
> session1 执行:update account set balance = 800 where name = 'lilei';
>  session2 对该表任一行操作都会阻塞住 InnoDB的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则都会从行锁升级为
>
> 表锁

## 行锁分析

> 通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况
>
> + Innodb_row_lock_current_waits: 当前正在等待锁定的数量 
> + Innodb_row_lock_time: 从系统启动到现在锁定总时间长度
> +  Innodb_row_lock_time_avg: 每次等待所花平均时间 
> + Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花时间
> + Innodb_row_lock_waits:系统启动后到现在总共等待的次数

## 查看INFORMATION_SCHEMA系统库锁相关数据表

>  ‐‐ 查看事务
>  select * from INFORMATION_SCHEMA.INNODB_TRX;
>  ‐‐ 查看锁
>  select * from INFORMATION_SCHEMA.INNODB_LOCKS;
>  ‐‐ 查看锁等待
>  select * from INFORMATION_SCHEMA.INNODB_LOCK_WAITS;
>
>  ‐‐ 释放锁，trx_mysql_thread_id可以从INNODB_TRX表里查看到
>  kill trx_mysql_thread_id
>
>  ‐‐ 查看锁等待详细信息
>  show engine innodb status\G;

## 锁优化建议

+ 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁 
+ 合理设计索引，尽量缩小锁的范围
+ 尽可能减少检索条件范围，避免间隙锁 
+ 尽量控制事务大小，减少锁定资源量和时间长度，涉及事务加锁的sql尽量放在事务最后执行 
+ 尽可能低级别事务隔离

# 4、sql join原理?

​		MySQL是只支持一种Join算法Nested-Loop Join(嵌套循环连接)，并不支持哈希连接和合并连接，不过在mysql中包含了多种变种，能够帮助MySQL提高join执行的效率。

​		**1、Simple Nested-Loop Join**

​		这个算法相对来说就是很简单了，从驱动表中取出R1匹配S表所有列，然后R2，R3,直到将R表中的所有数据匹配完，然后合并数据，可以看到这种算法要对S表进行RN次访问，虽然简单，但是相对来说开销还是太大了。

​		**2、Index Nested-Loop Join**

​		索引嵌套联系由于非驱动表上有索引，所以比较的时候不再需要一条条记录进行比较，而可以通过索引来减少比较，从而加速查询。这也就是平时我们在做关联查询的时候必须要求关联字段有索引的一个主要原因。

​		这种算法在链接查询的时候，驱动表会根据关联字段的索引进行查找，当在索引上找到了符合的值，再回表进行查询，也就是只有当匹配到索引以后才会进行回表。至于驱动表的选择，MySQL优化器一般情况下是会选择记录数少的作为驱动表，但是当SQL特别复杂的时候不排除会出现错误选择。

​		在索引嵌套链接的方式下，如果非驱动表的关联键是主键的话，这样来说性能就会非常的高，如果不是主键的话，关联起来如果返回的行数很多的话，效率就会特别的低，因为要多次的回表操作。先关联索引，然后根据二级索引的主键ID进行回表的操作。这样来说的话性能相对就会很差。

​		**3、Block Nested-Loop Join**

​		在有索引的情况下，MySQL会尝试去使用Index Nested-Loop Join算法，在有些情况下，可能Join的列就是没有索引，那么这时MySQL的选择绝对不会是最先介绍的Simple Nested-Loop Join算法，而是会优先使用Block Nested-Loop Join的算法。

​		Block Nested-Loop Join对比Simple Nested-Loop Join多了一个中间处理的过程，也就是join buffer，使用join buffer将驱动表的查询JOIN相关列都给缓冲到了JOIN BUFFER当中，然后批量与非驱动表进行比较，这也来实现的话，可以将多次比较合并到一次，降低了非驱动表的访问频率。也就是只需要访问一次S表。这样来说的话，就不会出现多次访问非驱动表的情况了，也只有这种情况下才会访问join buffer。

​		在MySQL当中，我们可以通过参数join_buffer_size来设置join buffer的值，然后再进行操作。默认情况下join_buffer_size=256K，在查找的时候MySQL会将所有的需要的列缓存到join buffer当中，包括select的列，而不是仅仅只缓存关联列。在一个有N个JOIN关联的SQL当中会在执行时候分配N-1个join buffer。

## 对于关联sql的优化

> + 关联字段加索引，让mysql做join操作时尽量选择index nested Loop join算法
> + 让小表驱动大表，写多表连接sql时如果明确知道哪张表是小表可以用straight_join写法固定连接驱动方式，省去mysql优化器自己判断的时间

## 小表驱动大表

> in和exsits优化
>
> + in:当B表的数据集小于A表的数据集时，in优于exists
>
>   select * from A where id in (select id from B)
>
> + A表的数据集小于B表的数据集时，exists优于in
>   select * from A where exists (select 1 from B where B.id = A.id)



# 5、MYsql索引失效的情况？



## 索引失效的情况：

组合索引 （name,age）

​		1、组合索引不遵循最左匹配原则 

​		2、组合索引的前面索引列使用范围查询(<,>,like),会导致后续的索引失效

​		3、不要在索引上做任何操作（计算，函数，类型转换）

​		4、is null和is not null无法使用索引

​		5、尽量少使用or操作符，否则连接时索引会失效

​		6、字符串不添加引号会导致索引失效

​		7、两表关联使用的条件字段中字段的长度、编码不一致会导致索引失效(之前创建表中编码，触发隐式转换)

​		8、like语句中，以%开头的模糊查询

​		9、如果mysql中使用全表扫描比使用索引快，也会导致索引失效

# 6、mysql如何做分库分表的？

​		使用mycat或者shardingsphere中间件做分库分表，选择合适的中间件，水平分库，水平分表，垂直分库，垂直分表

​		在进行分库分表的时候要尽量遵循以下原则：

​		1、能不切分尽量不要切分；

​		2、如果要切分一定要选择合适的切分规则，提前规划好；

​		3、数据切分尽量通过数据冗余或表分组来降低跨库 Join 的可能；

​		4、由于数据库中间件对数据 Join 实现的优劣难以把握，而且实现高性能难度极大，业务读取尽量少使用多表 Join。

> **通俗点说垂直分库就是“大表拆小表”，基于列字段进行的**
>
>  拆分原则一般是表中的字段较多，将不常用的或者数据较大，长度较长的拆分到“扩展表 如text类型字段
>    访问频次低、字段大的商品描述信息单独存放在一张表中
>    访问频次较高的商品基本信息单独放在一张表中
>
> **一般从单体项目升级改造为微服务项目，就是垂直分库**

# 7、数据存储引擎有哪些？

​		大家可以通过show engines的方式查看对应的数据库支持的存储引擎。

InnoDB、MyISAM、memort、Performance_schema(监控)

# 8、描述一下InnoDB和MyISAM的区别？

| 区别     | Innodb                         | MyISAM                             |
| -------- | ------------------------------ | ---------------------------------- |
| 事务     | 支持                           | 不支持                             |
| 外键     | 支持                           | 不支持                             |
| 索引     | 即支持聚簇索引又支持非聚簇索引 | 只支持非聚簇索引                   |
| 行锁     | 支持                           | 不支持                             |
| 表锁     | 支持                           | 支持                               |
| 存储文件 | frm，ibd(数据文件+索引文件)    | frm,myi(索引文件),myd(数据文件)    |
| 具体行数 | 每次必须要全表扫描统计行数     | 通过变量保存行数（查询不能带条件） |

如何选择？

​		1、是否需要支持事务，如果需要选择innodb，如果不需要选择myisam

​		2、如果表的大部分请求都是读请求，可以考虑myisam，如果既有读也有写，使用innodb

​		现在mysql的默认存储引擎已经变成了Innodb,推荐使用innodb

# 9、描述一下聚簇索引和非聚簇索引的区别？

​		innodb存储引擎在进行数据插入的时候必须要绑定到一个索引列上，默认是主键，如果没有主键，会选择唯一键，如果没有唯一键，那么会选择生成6字节的rowid，跟数据绑定在一起的索引我们称之为聚簇索引，没有跟数据绑定在一起的索引我们称之为非聚簇索引。

​		innodb存储引擎中既有聚簇索引也有非聚簇索引，而myisam存储引擎中只有非聚簇索引。

Innodb 存储文件ibd(数据+ 文件),所以这个是聚簇索引也有非聚簇索引，（非聚簇索引）：如果包含多个，不和数据关联的索引就是非聚簇索引

myisam myi(索引文件),myd(数据文件)

# 11、描述一下mysql主从复制的机制的原理？mysql主从复制主要有几种模式？

​		参考mysql主从复制原理文档

![image-20220708004530667](https://cdn.wuzx.cool/image-20220708004530667.png)

# 13、MySQL为什么选择B+树作为它的存储结构，为什么不选择Hash、二叉、红黑树？

## 二叉树

![image-20220824160444665](https://cdn.wuzx.cool/image-20220824160444665.png)

![image-20220824160502012](https://cdn.wuzx.cool/image-20220824160502012.png)

![image-20220824160534262](https://cdn.wuzx.cool/image-20220824160534262.png)

> 二叉树的查询效率在理想的情况下是o(log n),但是如果出现单边增长的一个情况，那么二叉树就退化成了链表，查询效率是o(n)

## 平衡二叉搜索树 (Balanced binary search trees,又称**AVL树**、平衡二叉查找树)

> - 具有二叉查找树的特点(左子树任一节点小于父节点，右子树任一节点大于父节点)，任何一个节点的左子树与右子树都是平衡二叉树
> - 任一节点的左右子树高度差小于1，即平衡因子为范围为[-1,1] 
>
> ### **为什么选择AVL树而不是BST？**
>
> + 大多数的BST的操作(如搜索、最大值、最小值、插入、删除等)的时间复杂度为O(h)，其中h是BST的高度。对于极端情况下的二叉树，这些操作的成本可能变为O(n)。如果确保每次插入和删除后树的高度都保持O(log n)，则可以保证所有这些操作的效率都是O(log n)，所以选择AVL树

## 红黑树(Red - Black Tree)

> AVL树比红黑树更加平衡，但AVL树可能在插入和删除过程中引起更多旋转。因此，如果应用程序涉及许多频繁的插入和删除操作，则应首选Red Black树(如 Java 1.8中的HashMap)。如果插入和删除操作的频率较低，而搜索操作的频率较高，则AVL树应优先于红黑树
>
> 但是我们的表数据意向都是很多，所以就算使用红黑树作为数据结构，树的高度会随着数据的越来越多而越来越高，所以还是会有很多的IO

## B树

B-树,这里的 B 表示 balance( 平衡的意思),B-树是一种多路自平衡的搜索树（B树是**一颗多路平衡查找树**），B树通过在节点中放置最大可能的键来保持B树的高度较低，B-树的每个节点是 n 个有序的序列(a1,a2,a3…an)，并将该节点的子节点分割成 n+1 个区间来进行索引(X1< a1, a2 < X2 < a3, … , an+1 < Xn < anXn+1 > an)。

![image-20220824164008487](https://cdn.wuzx.cool/image-20220824164008487.png)

+ 所有索引元素不重复,索引值和具体data都在每个节点里
+ 节点中的数据索引从左到右递增排列
+ 叶节点具有相同的深度，叶节点的前后之前指针为空

### B树查询流程

> + 若搜索 key 为 60 节点的 data，首先在根节点进行二分查找（每个节点都是排好序的），判断60> 25 ,然后找到同一个节点的下一个key比较，找到 60 在【50-75】然后找到子树节点，
> + 将子树节点读取。此时进行一次磁盘 IO，将该节点从磁盘读入内存
> + 接着继续进行上述过程，直到找到该 key 为止。

## B+ 树

B+树是B-树的变体，也是一种多路搜索树, 它与 B- 树的不同之处在于:

![image-20220824172834885](https://cdn.wuzx.cool/image-20220824172834885.png)

+ 非叶子节点不存储data，只存储索引(冗余)，可以放更多的索引
+ 叶子节点包含所有索引字段
+ 叶子节点用指针连接，提高区间访问的性能，**可使用在范围查询等**

## Hash

![image-20220824173138053](https://cdn.wuzx.cool/image-20220824173138053.png)

+ 对索引的key进行一次hash计算就可以定位出数据存储的位置
+ 很多时候Hash索引要比B+ 树索引更高效
+ 仅能满足 “=”，“IN”，不支持范围查询
+ 会产生hash冲突问题

# 14、描述一下mysql的乐观锁和悲观锁，锁的种类？

​		乐观锁并不是数据库自带的，如果需要使用乐观锁，那么需要自己去实现，一般情况下，我们会在表中新增一个version字段，每次更新数据version+1,在进行提交之前会判断version是否一致。

​		mysql中的绝大部分锁都是悲观锁，按照粒度可以分为行锁和表锁：

​		**行锁：**

​			共享锁：当读取一行记录的时候，为了防止别人修改，则需要添加S锁

​			排它锁：当修改一行记录的时候，为了防止别人同时进行修改，则需要添加X锁

|      |   X    |   S    |
| :--: | :----: | :----: |
|  X   | 不兼容 | 不兼容 |
|  S   | 不兼容 |  兼容  |

​			记录锁：添加在行索引上的锁

​			间隙锁：锁定范围是索引记录之间的间隙，针对可重复读以上隔离级别

​			临键锁：记录锁+间隙锁

​		**表锁：**

​			意向锁：在获取某行的锁之前，必须要获取表的锁，分为意向共享锁，意向排它锁

​			自增锁：对自增字段所采用的特殊表级锁

​		锁模式的含义：

​			IX：意向排它锁

​			X：锁定记录本身和记录之前的间隙

​			S：锁定记录本身和记录之前的间隙

​			X,REC_NOT_GAP：只锁定记录本身

​			S，REC_NOT_GAP：只锁定记录本身

​			X，GAP：间隙锁，不锁定记录本身

​			S，GAP：间隙锁，不锁定记录本身

​			X，GAP,INSERT_INTENTION：插入意向锁

# 15、mysql原子性和持久性是怎么保证的？

​		原子性通过undolog来实现，持久性通过redo log来实现

# 16、描述一下mysql一条数据是如何保存到数据库的

![image-20220708011639622](https://cdn.wuzx.cool/image-20220708011639622.png)

> + 客户端提交sql到server，将执行语句加载到内存中
> + Select 需要优先将磁盘的数据加载到内存
> + update、delete、insert 读取数据结果放在内存中，unlog(回滚)、redolog、binlog保证数据一致性两阶段提交，然后数据溢写到磁盘

# 17、 说一下mysql调优

之前做过很多mysql优化点，优化其实不是问题出现再做优化，在做优化之前会做一些预防，比如表的设计，schema的设计等相关问题，肯定是要准备的。在实际的生产过程中，更多的问题是运行一段时间后，我们通过profile，profermance_schemad等相关方式来监控sql语句及数据库的状态，当出现问题会从执行计划、索引、sql优化、参数调整。像我之前做的xx项目，出现了xx问题，我们通过排查，找到原因最终发现是XX问题，当添加XX后修改XX之后就解决了，其实sql优化最主要的是思路，我们解决问题的时候从来不是上来就优化，而是找准问题的关键。

# 18.mysql 索引数据结构

> 索引是帮助Mysql高效获取数据的排好序的数据结构

## 索引的数据结构

### 二叉树

![image-20220710123302067](../../Library/Application%20Support/typora-user-images/image-20220710123302067.png)

> 每一个字段key(索引字段的值) ，value (索引所在行的磁盘文件地址)，二叉树的根节点大于左孩子，小于右孩子，从根节点开始查找，对比全表扫描效率稍微会高一点,但是对于单边增长的数据列,例如（1,2,3,4,5,6,7）这样的数据，树会退化成链，所以I/O的次数和全表查找一样，所以可以考虑红黑树

### 红黑树(解决单边数据增长问题)

> 平衡二叉树：
>
> + AVL的左右子树高度差不能超过1，每次进行插入/删除操作时，几乎都需要通过旋转操作保持平衡
> + 在频繁进行插入/删除的场景中，频繁的旋转操作使得AVL的性能大打折扣
> + 红黑树通过牺牲严格的平衡，换取插入/删除时少量的旋转操作，**整体性能**优于AVL
>
>  平衡二叉树和红黑树的区别：
>
> - 平衡二叉树的左右子树的高度差绝对值不超过1，但是红黑树在某些时刻可能会超过1，只要符合红黑树的五个条件即可。
> - 二叉树只要不平衡就会进行旋转，而红黑树不符合规则时，有些情况只用改变颜色不用旋转，就能达到平衡

但是如果数据很大，就会造成树的高度很高，所以I/O次数肯定也会很高，我们就会想在怎么才可以把树的高度减少，然后就出现了

### Hash表

+ 对索引的key进行一次hash计算定位出数据存储的位置

+ 很多hash索引要比B+ tree树索引更高效

+ 仅能满足"=" ,"in" 不支持范围查询

+ hash冲突问题

  ![](https://cdn.wuzx.cool/image-20220710234621649.png)



### B-Tree

![image-20220710125543061](https://cdn.wuzx.cool/image-20220710125543061.png)

> + 叶子节点具有相同的深度，叶子节点的指针为空
> + 所有索引元素不重复
> + 节点中的数据索引从左到右递增排列

### B+Tree

![image-20220710125647989](https://cdn.wuzx.cool/image-20220710125647989.png)

> + 非叶子节点不存储data,只存储索引(冗余)，可以放更多的索引
> + 叶子节点包含所有的索引字段
> + 叶子节点用指针连接，提高区间访问的性能

``` mysql
show GLOBAL STATUS like '%INNODB_page_size%' 设置页的大小
```

+ 每一个非叶子节点可以存放更多的索引元素，所以树的高度就更小

# 19.为什么建议innoDB必须建议主键，并推荐使用整形的自增主键

## innoDB为什么必须要有主键

> + 如果设置了主键，就会使用主键作为聚集索引。
> + 如果你没有创建主键，他会寻找一个不为null且唯一的字段作为主键索引。减少性能消耗
> + 如果还是没有则会隐式的创建一个字段作为主键row_id 

## 为什么使用整型

> 整型容易比较大小进行排序，B+tree中的[叶子节点](https://so.csdn.net/so/search?q=叶子节点&spm=1001.2101.3001.7020)和非叶子节点同层中都是从左到右有序递增的

## 为什么要求自增

> mysql为了维护索引的有序性，使得新增的主键会在B+tree叶子节点中最后末尾添加，使得树分叉概率变小

# 20. 为什么非主键索引结构叶子节点存储的是主键值

![image-20220711000811980](https://cdn.wuzx.cool/image-20220711000811980.png)

+ 保持一致性，主键先插入成功，然后再插入非聚集索引
+ 节约存储空间

# 21.联合索引的底层数据结构

![image-20220711001526088](https://cdn.wuzx.cool/image-20220711001526088.png)

+ 因为这个索引的是按这个排好序的结构，如果直接是`age`查找是找不到的

# 22 EXPLAIN 详解

![image-20220726223100575](https://cdn.wuzx.cool/image-20220726223100575.png)

## 1. id

> id就是select的序列号
>
> + id越大执行的优先级越高
> + id相同，从上到下执行
> + id为unll，最后执行

## 2. select_type

查询对应的类型

+ `simple`:简单查询，不包含子查询和union

+ `primary`查询中若包含任何复杂的子查询，最外层查询则被标记为Primary

  > explain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ;

+ `union`:若第二个select出现在union之后，则被标记为union
  explain select * from emp where deptno = 10 union select * from emp where sal >2000

+ `DERIVED`from子句中出现的子查询，也叫做派生类，

  explain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno

+ `subquery`:在select或者where列表中包含子查询

  explain select * from emp where sal > (select avg(sal) from emp) ;

## 3.table

对应行正在访问哪一个表，表名或者别名，可能是临时表或者union合并结果集

1. 如果是具体的表名，则表明从实际的物理表中获取数据，当然也可以是表的别名
2. 表名是derivedN的形式，表示使用了id为N的查询产生的衍生表
3. 当有union result的时候，表名是union n1,n2等的形式，n1,n2表示参与union的id

## 4.type

表示的是访问类型，就是说明我们以什么样的方式去访问数据,一般来说保证优化到ref，range也是可以的

`system > const

+ `system`表中只有一行数据，是const的一种特例。
+ `const`这个表至多有一个匹配行
+ `eq_ref`primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type
+ `ref`:使用普通所有或者使用唯一索引的前缀，索引和某个值相比较，找到多个符合条件的行
+ `range`:范围扫描通常出现在 in(), between ,> ,<, >= 等操作中。使用一个索引来检索给定范围的行。
+ `index`:扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接 对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这 种通常比ALL快一些
+ `ALL`即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了

## 5. possible_keys列

这一列显示查询可能使用哪些索引来查找。

## 6.key

> 这一列显示mysql实际采用那个所有来对数据表进行访问

## 7.key_len

mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列

## 8.ref

显示索引的哪一列被使用了，如果可能的话，是一个常数

## 9. rows

这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。

## 10.Extra

+ `using index`:这个表示当前的查询时覆盖索引的，直接从索引中读取数据，而不用访问数据表。如果同时出现using where 表名索引被用来执行索引键值的查找，如果没有，表面索引被用来读取数据，而不是真的查找
  explain select film_id from film_actor where film_id = 1;

+ `Using where`:使用 where 语句来处理结果，并且查询的列未被索引覆盖

   explain select * from actor where name = 'a';

+ `Using index condition`:查询的列不完全被索引覆盖，where条件中是一个前导列的范围;

+ `Using temporary`:mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索 引来优化

+ `Using filesort`:将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一 般也是要考虑使用索引来优化的

# 23.索引优化最佳实践

+ `全值匹配`,  索引生效，执行效率高。
+ `最左前缀法则`，联合索引，  从索引的最左前列开始并且不跳过索引中的列。
+ `不在索引列上做任何操作(计算、函数、(自动or手动)类型转换)，会导致索引失效而转向全表扫描`,
+ 组合索引的前面索引列使用范围查询(<,>,like),会导致后续的索引失效
+ 尽量使用覆盖索引(只访问索引的查询(索引列包含查询列))，减少 select * 语句
+ mysql在使用不等于(!=或者<>)，not in ，not exists 的时候无法使用索引会导致全表扫描
   < 小于、 > 大于、 <=、>= 这些，mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引
+ is null,is not null 一般情况下也无法使用索引
+ like以通配符开头('$abc...')mysql索引失效会变成全表扫描操作
+ 字符串不加单引号索引失效
+ 尽量少使用or操作符，否则连接时索引会失效
+ 两表关联使用的条件字段中字段的长度、编码不一致会导致索引失效(之前创建表中编码，触发隐式转换)

![image-20220727224410352](https://cdn.wuzx.cool/image-20220727224410352.png)

# 24、什么是索引下推

> 对于辅助的联合索引(name,age,position)，正常情况按照最左前缀原则，SELECT * FROM employees WHERE name like 'LiLei%' AND age = 22 AND position ='manager' 这种情况只会走name字段索引，因为根据name字段过滤完，得到的索引行里的age和 position是无序的，无法很好的利用索引。
>
> + Mysql5.6之前，这个查询只能在联合索引里匹配到名字是 'LiLei' 开头的索引，然后根据对应主键的id逐个回表查询，到主键索 引上找出相应的记录，再比对age和position这两个字段的值是否符合。
> + Mysql5.6之后，在索引遍历的过程中，找到 'LiLei' 开头的索引并且去过滤，过滤之后最后再回表。索引下推有效的减少回表次数。

## 为什么范围查找Mysql没有用索引下推优化?

​	估计应该是Mysql认为范围查找过滤的结果集过大，like KK% 在绝大多数情况来看，过滤后的结果集比较小，所以这里Mysql选择给 like KK% 用了索引下推优化，当然这也不是绝对的，有时like KK% 也不一定就会走索引下推。

# 25、常见sql深入优化

## 1.Order by与Group by优化

> 、MySQL支持两种方式的排序filesort和index，Using index是指MySQL扫描索引本身完成排序。index
>
> 效率高，filesort效率低。
>  2、order by满足两种情况会使用Using index。
>
> \1) order by语句使用索引最左前列。
>
> \2) 使用where子句与order by子句条件列组合满足索引最左前列。 3、尽量在索引列上完成排序，遵循索引建立(索引创建的顺序)时的最左前缀法则。
>  4、如果order by的条件不在索引列上，就会产生Using filesort。
>  5、能用覆盖索引尽量用覆盖索引
>  6、group by与order by很类似，其实质是先排序后分组，遵照索引创建顺序的最左前缀法则。对于group by的优化如果不需要排序的可以加上order by null禁止排序。注意，where高于having，能写在where中 的限定条件就不要去having限定了。

## 2. Using filesort文件排序原理详解

+ filesort文件排序方式
  + 单路排序：一次性取出所有的字段，然后再sort buffer中进行排序。
  + 双路排序(回表排序)：首先根据相应的条件取出排序字段和直接可以定位数据行的id，然后再sort buffer中排序，排序完之后再次回表取出其他字段。
+ MySQL 通过比较系统变量 max_length_for_sort_data(默认1024字节) 的大小和需要查询的字段总大小来 判断使用哪种排序模式。
  + 如果 字段的总长度小于max_length_for_sort_data ，那么使用 单路排序模式; 
  + 如果 字段的总长度大于max_length_for_sort_data ，那么使用 双路排序模∙式。

# 26.索引设计原则

## 1、代码先行，索引后上

> 不知大家一般是怎么给数据表建立索引的，是建完表马上就建立索引吗? 这其实是不对的，一般应该等到主体业务功能开发完毕，把涉及到该表相关sql都要拿出来分析之后再建立 索引。

## 2、联合索引尽量覆盖条件

> 比如可以设计一个或者两三个联合索引(尽量少建单值索引)，让每一个联合索引都尽量去包含sql语句里的 where、order by、group by的字段，还要确保这些联合索引的字段顺序尽量满足sql查询的最左前缀原 则。

## 3、不要在小基数字段上建立索引

> 索引基数是指这个字段在表里总共有多少个不同的值，比如一张表总共100万行记录，其中有个性别字段， 其值不是男就是女，那么该字段的基数就是2。 如果对这种小基数字段建立索引的话，还不如全表扫描了，因为你的索引树里就包含男和女两种值，根本没 法进行快速的二分查找，那用索引就没有太大的意义了。 一般建立索引，尽量使用那些基数比较大的字段，就是值比较多的字段，那么才能发挥出B+树快速二分查 找的优势来。

## 4.长字符串我们可以采用前缀索引

> 尽量对字段类型较小的列设计索引，比如说什么tinyint之类的，因为字段类型较小的话，占用磁盘空间也会 比较小，此时你在搜索的时候性能也会比较好一点。
>
> 对于这种varchar(255)的大字段可能会比较占用磁盘空间，可以稍微优化下，比如针对这个字段的前20个 字符建立索引
>
> 对这个字段里的每个值的前20个字符放在索引树里，类似于 KEY index(name(20),age,position)
>
> 此时你在where条件里搜索的时候，如果是根据name字段来搜索，那么此时就会先到索引树里根据name 字段的前20个字符去搜索，定位到之后前20个字符的前缀匹配的部分数据之后，再回到聚簇索引提取出来 完整的name字段值进行比对。
>  但是假如你要是order by name，那么此时你的name因为在索引树里仅仅包含了前20个字符，所以这个排 序是没法用上索引的， group by也是同理。所以这里大家要对前缀索引有一个了解

## 5、where与order by冲突时优先where

> 在where和order by出现索引设计冲突时，到底是针对where去设计索引，还是针对order by设计索引?到 底是让where去用上索引，还是让order by用上索引?
>
> 一般这种时候往往都是让where条件去使用索引来快速筛选出来一部分指定的数据，接着再进行排序。 因为大多数情况基于索引进行where筛选往往可以最快速度筛选出你要的少部分数据，然后做排序的成本可 能会小很多。

## 6、基于慢sql查询做优化

> 可以根据监控后台的一些慢sql，针对这些慢sql查询做特定的索引优化。 关于慢sql查询不清楚的可以参考这篇文章:https://blog.csdn.net/qq_40884473/article/details/89455740

# 27、count(*)查询优化

``` shell
mysql> EXPLAIN select count(1) from employees;
mysql> EXPLAIN select count(id) from employees;
mysql> EXPLAIN select count(name) from employees;
mysql> EXPLAIN select count(*) from employees;
```

+ 以上4条查询只有根据某个字段查询是不会统计null值的数据行
+ 四个sql的执行计划一样，说明这四个sql执行效率应该差不多

字段有索引，count(*) ≈ count(1) > count(字段) > count(id) ,字段有索引，count(字段)走的是二级索引，二级索引存储的数据比主键索引少，所以count(字段) > count(id)
字段无索引:count(*)≈count(1)>count(主键 id)>count(字段) //字段没有索引count(字段)统计走不了索引， count(主键 id)还可以走主键索引，所以count(主键 id)>count(字段)

count(1)跟count(字段)执行过程类似，不过count(1)不需要取出字段统计，就用常量1做统计，count(字段)还需要取出 字段，所以理论上count(1)比count(字段)会快一点。

count(*) 是例外，mysql并不会把全部字段取出来，而是专门做了优化，不取值，按行累加，效率很高，所以不需要用 count(列名)或count(常量)来替代 count(*)。



## 常见优化方法

+ 查询mysql自己维护的总行数

  > 对于myisam存储引擎的表做不带where条件的count查询性能是很高的，因为myisam存储引擎的表的总行数会被 mysql存储在磁盘上，查询不需要计算

+ show table status

  > 如果只需要知道表总行数的估计值可以用如下sql查询，性能很高

+ 将总数维护到Redis里

  > 插入或删除表数据行的时候同时维护redis里的表总行数key的计数值(用incr或decr命令)，但是这种方式可能不准，很难 保证表操作和redis操作的事务一致性

+ 增加数据库计数表

  > 插入或删除表数据行的时候同时维护计数表，让他们在同一个事务里操作

# MYSQL 三大日志(binlog、redo log、undo log)

## redo log(重做日志)

![image-20220815163806035](https://cdn.wuzx.cool/image-20220815163806035.png)

+ mysql 中数据以`页`为单位，innodb_page_size 每页是16kb，每次修改数据都是将每页从硬盘查询出来，放在`Buffer pool`中。后续的查询都是从`buffer pool`中查询，如果没有命中再去磁盘中查询
+ 更新表的数据是直接更新`buffer pool` 中的数据,直接在`buffer pool`中更新
+ 在某个数据页上做了什么修改，记录到 `redo log buffer`,后面·清空redo log buffer刷盘到 redo log 日志文件中

### redo log 刷盘

`Innodb`存储引擎为`redo log`的刷盘有三种策略，可以通过参数`innodb_flush_log_at_trx_commit`设置

+ 0：设置为0，每次事务提交不进行刷盘
+ 1：设置为1，每次事务提交都进行刷盘(默认),也就是说当事务提交时会调用 `fsync` 对 redo log 进行刷盘
+ 2：设置为2，每次事务提交都只是将`redo log buffer`内容写入`page cache`

![image-20220824175223918](https://cdn.wuzx.cool/image-20220824175223918.png)

`InnoDB` 存储引擎有一个后台线程，每隔`1` 秒，就会把 `redo log buffer` 中的内容写到文件系统缓存（`page cache`），然后调用 `fsync` 刷盘。

### innodb_flush_log_at_trx_commit=0

![image-20220824175540279](https://cdn.wuzx.cool/image-20220824175540279.png)

> 每次事务提交，不进行刷盘，如果mysql 宕机，那么会丢失1s内的数据，因为有一个后台线程会每隔1s将redo log buffer 内容写入到page cache，然后调用fsync进行刷盘

### innodb_flush_log_at_trx_commit=1(mysql默认的刷盘策略)

![image-20220824175814138](https://cdn.wuzx.cool/image-20220824175814138.png)

> 当`innodb_flush_log_at_trx_commit=1`,redo log 记录一定在硬盘中，因为事务提交，就会进行主动刷盘，就算mysql宕机，因为事务没有提交，所以这部分日志丢失也没有关系

###  innodb_flush_log_at_trx_commit=2

![image-20220824180026077](https://cdn.wuzx.cool/image-20220824180026077.png)

> 为`2`时， 只要事务提交成功，`redo log buffer`中的内容只写入文件系统缓存（`page cache`）
>
> 如果仅仅只是`MySQL`挂了不会有任何数据丢失，但是宕机可能会有`1`秒数据的丢失。



## binlog

> 二进制日志`binlog`可以说是MySQL最重要的日志，属于`MYSQL Server层`它记录了所有的DDL和DML语句（除了数据查询语句select）,以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的
>
> + DDL(Data Definition Language) 数据库定义语言 ，主要的命令有create、alter、drop等，ddl主要是用在定义或改变表(table)的结构,数据类型，表之间的连接和约束等初始工作上，他们大多在建表时候使用
> + DML(Data Manipulation Language) 数据操纵语言,主要命令是select,update,insert,delete,就像它的名字一样，这4条命令是用来对数据库里的数据进行操作的语言

### 使用场景

![image-20220825145131094](https://cdn.wuzx.cool/image-20220825145131094.png)

+ 主从复制
+ 数据恢复，可以通过mysqlbinlog工具来进行数据恢复

### 记录格式

`binlog`日志有三种格式，可以通过`binlog_format`参数指定`statement`、`row`、`mixed`

#### statement

> 指定`statement`，记录的内容是`SQL`语句原文，比如执行一条`update T set update_time=now() where id=1`，记录的内容如下。
>
> ![image-20220825145756091](https://cdn.wuzx.cool/image-20220825145756091.png)
>
> 同步数据时，会执行记录的`SQL`语句，但是有个问题，`update_time=now()`这里会获取当前系统时间，直接执行会导致与原库的数据不一致。
>
> 为了解决这个问题可以指定为`row`

#### row

> 指定`row`,记录的内容不再是简单的`SQL`,还包含具体数据
>
> ![image-20220825150230410](https://cdn.wuzx.cool/image-20220825150230410.png)`row`格式记录的内容看不到详细信息，要通过`mysqlbinlog`工具解析出来。
>
> `update_time=now()`变成了具体的时间`update_time=1627112756247`，条件后面的@1、@2、@3 都是该行数据第 1 个~3 个字段的原始值（**假设这张表只有 3 个字段**）。
>
> 这样就能保证同步数据的一致性，通常情况下都是指定为`row`，这样可以为数据库的恢复与同步带来更好的可靠性。
>
> 但是这种格式，需要更大的容量来记录，比较占用空间，恢复与同步时会更消耗`IO`资源，影响执行速度。
>
> 所以就有了一种折中的方案，指定为`mixed`，记录的内容是前两者的混合。
>
> `MySQL`会判断这条`SQL`语句是否可能引起数据不一致，如果是，就用`row`格式，否则就用`statement`格式

## binlog 二进制日志写入

binlog写入，先将binlog日志写入到`binlog cache`，事务提交的时候，把`binlog cache`写入到`binlog`文件中。因为一个事务的binlog不能被拆开，所以无论这个事务多大，都要一次性写入，所以给每个线程都分配binlog cache,可以通过 `binlog_cache_size`设置单个线程`binlog cache`大小

![image-20220825151940943](https://cdn.wuzx.cool/image-20220825151940943.png)

+ **write，是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快**
+ **sync，才是将数据持久化到磁盘的操作**
+ `write`和`fsync`的时机，可以由参数`sync_binlog`控制，默认是`0`
    + `sync_binlog = 0`,每次提交事务，都只是`write`将binglog写入到`page cache`中,由系统判断什么时候 `fsync`
    + `sync_binlog =1 `,每次提交事务，都会执行`fsync`，安全起见可以设置为1
    + `sync_binlog =N`表示每次提交事务都`write`，但累积`N`个事务后才`fsync`

## 两阶段提交

在执行更新语句过程，会记录`redo log`与`binlog`两块日志，以基本的事务为单位，`redo log`在事务执行过程中可以不断写入，而`binlog`只有在提交事务时才写入，所以`redo log`与`binlog`的写入时机不一样，

### `redo log`与`binlog`两份日志之间的逻辑不一致，会出现什么问题？

> 假设`id=2`的记录，字段`c`值是`0`，把字段`c`值更新成`1`，`SQL`语句为`update T set c=1 where id=2`。假设执行过程中写完`redo log`日志后，`binlog`日志写期间发生了异常
>
> ![image-20220825154248895](https://cdn.wuzx.cool/image-20220825154248895.png)

由于`binlog`没写完就异常，这时候`binlog`里面没有对应的修改记录。因此，之后用`binlog`日志恢复数据时，就会少这一次更新，恢复出来的这一行`c`值是`0`，而原库因为`redo log`日志恢复，这一行`c`值是`1`，最终数据不一致

![image-20220825154304997](https://cdn.wuzx.cool/image-20220825154304997.png)

### 为了解决两份日志之间的逻辑一致问题，`InnoDB`存储引擎使用**两阶段提交**方案。

> 原理很简单，将`redo log`的写入拆成了两个步骤`prepare`和`commit`，这就是**两阶段提交**。

![image-20220825154522376](https://cdn.wuzx.cool/image-20220825154522376.png)

#### redo log 处于prepare阶段宕机

![image-20220825154624933](https://cdn.wuzx.cool/image-20220825154624933.png)

+ 如果是在prepare阶段宕机，那么mysql重启的时候会判断redo log是否commit状态，如果是commit 直接根据redolog提交事务，如果是否，则根据事务id找到binlog日志，可以找到就提交事务，恢复数据

### `redo log`设置`commit`阶段发生异常，那会不会回滚事务

![image-20220825154931214](https://cdn.wuzx.cool/image-20220825154931214.png)

MySQL InnoDB 引擎使用 **redo log(重做日志)** 保证事务的**持久性**，使用 **undo log(回滚日志)** 来保证事务的**原子性**。