https://www.cnblogs.com/heqiyoujing/p/11146178.html

# 1.倒排索引

## 1. 1 什么是倒排索引



> 首先会给要创建索引的字段拆分若干个词项(term dictionary)，按照字典续排列。 Posting list(i倒排表)存放的是包含该此项的所有id，从小到大有序数据。
>
> `term index`:词项索引，为了加速term dictionary的数据结构
>
> `term dictionary`:词项字典
>
> `posting List`倒排表

![image-20220711005933801](https://cdn.wuzx.cool/image-20220711005933801.png)

全文检索： 首先通过吧搜索词，按照一定的分词策略，拆分成词项，然后去倒排表里面检索，去查看`term dictionary`是否命中，如果命中就会获取一个匹配的标记，然后取出`posting list` 的id列表

## 1.2  倒排索引是用来解决哪些问题的

> 大量的数据搜索,

## 1.3 什么是字典树

字典树（TrieTree），是一种树形结构，典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串,如01字典树）。主要思想是利用字符串的公共前缀来节约存储空间。很好地利用了串的公共前缀，节约了存储空间。字典树主要包含两种操作，插入和查找。

比如，我们要怎么用树存下单词"abc",“abb”,“bca”,"bc"呢
![image-20220711012536307](https://cdn.wuzx.cool/image-20220711012536307.png)

### FSM (Finite State Machines)

![image-20220711012918810](https://cdn.wuzx.cool/image-20220711012918810.png)

### FSA(有限状态接收机)

> 在FSA的基础上，增加了`开始和结束`

### FAT(有限状态转换机)

> FST有两个优点：
>
> + 1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；
> + 2）查询速度快。O(len(str))的查询[时间复杂度](https://so.csdn.net/so/search?q=时间复杂度&spm=1001.2101.3001.7020)。

# 2.Elasticsearch的写入原理

+ `索引(index)` :Elasticsearch中的“索引”有点像关系数据库中的数据库。 它是存储/索引数据的地方

+ `分片(shard)`：“分片”是Lucene的一个索引的数据段。 它本身就是一个功能齐全的搜索引擎 , “索引”可以由单个分片组成，但通常由多个分片组成，一部分主分片、一部分副本分片. 

  + ES默认5个主分片，1个副本分片；
  + 副本分片的用途：（1）主节点故障时的故障转移；（2）增加的读取吞吐量。

+ `分段 segment`:每个分片包含多个“分段”，其中分段是倒排索引,**在分片中搜索将依次搜索每个片段，然后将其结果合并到该分片的最终结果中**

+ `translog日志文件`:为了防止elasticsearch宕机造成数据丢失保证可靠存储，es会将每次写入数据同时写到translog日志中。

  translog还用于提供实时CRUD。 当您尝试按ID检索，更新或删除文档时，它会首先检查translog中是否有任何最近的更改，然后再尝试从相关段中检索文档。 这意味着它始终可以实时访问最新的已知文档版本

![image-20220711020506504](https://cdn.wuzx.cool/image-20220711020506504.png)

## Elasticsearch写入步骤拆解

> + 新document首先写入内存Buffer缓存中
> + 每隔一段时间(1s)，buffer写入新Segment(倒排索引)中
> + 新segment写入文件系统缓存 filesystem cache
> + 当segment的阈值满的时候 触发“commitpoint”操作，filesystem cache(文件系统缓存)中的index segment被fsync强制刷到磁盘上，确保物理写入
> + 然后响应Segment文件将文件改成状态open,然后数据就可以查询
> + 清空内存buffer，可以接收新的文档写入

## Elasticsearch refresh和flush

### refresh操作

先将index-buffer中文档（document）生成的segment写到文件系统之中，这样避免了比较损耗性能io操作，又可以使搜索可见。
默认1s钟刷新一次，所以说ES是近实时的搜索引擎，不是准实时。
注意：实际需要结合自己的业务场景设置refresh频率值。调大了会优化索引速度。注意单位：s代表秒级

``` http
PUT /my_logs
{
  "settings": {
    "refresh_interval": "30s" 
  }
}
```

### flush操作

新创建的document数据会先进入到index buffer之后，与此同时会将操作记录在translog之中，当发生refresh时ranslog中的操作记录并不会被清除，而是当数据从filesystem cache中被写入磁盘之后才会将translog中清空。
从filesystem cache写入磁盘的过程就是flush

```
步骤1：当translog变得太大时 ，可以执行commit ponit操作。
步骤2：使用fsync刷新文件系统缓存，写入磁盘。
步骤3：旧缓冲区被清除。
```

操作

``` http
POST /_flush?wait_for_ongoing 
```

# 3. 读写性能调优

## 写入性能调优：

- 增加flush时间间隔，目的是减小数据写入磁盘的频率，减小磁盘IO

- 进来使用bulk批量写入

- 增加refresh_interval的参数值，目的是减少segment文件的创建，减少segment的merge次数，merge是发生在jvm中的，有可能导致full GC，增加refresh会降低搜索的实时性。

- 增加Buffer大小，本质也是减小refresh的时间间隔，因为导致segment文件创建的原因不仅有时间阈值，还有buffer空间大小，写满了也会创建。         默认最小值 48MB< 默认值 堆空间的10% < 默认最大无限制

- 大批量的数据写入尽量控制在低检索请求的时间段，大批量的写入请求越集中越好。

  - 第一是减小读写之间的资源抢占，读写分离
  - 第二，当检索请求数量很少的时候，可以减少甚至完全删除副本分片，关闭segment的自动创建以达到高效利用内存的目的，因为副本的存在会导致主从之间频繁的进行数据同步，大大增加服务器的资源占用。

- Lucene的数据的fsync是发生在OS cache的，要给OS cache预留足够的内从大小，详见JVM调优。

- 通用最小化算法，能用更小的字段类型就用更小的，keyword类型比int更快，

- ignore_above：字段保留的长度，越小越好

- 调整_source字段，通过include和exclude过滤

- store：开辟另一块存储空间，可以节省带宽

  ***\*注意：_\*******\*sourse\*******\*：\*******\*设置为false\*******\*，\*******\*则不存储元数据\*******\*，\*******\*可以节省磁盘\*******\*，\*******\*并且不影响搜索\*******\*。但是禁用_\*******\*source必须三思而后行\*******\*：\****

  \1. [update](https://www.elastic.co/guide/en/elasticsearch/reference/7.9/docs-update.html)，[update_by_query](https://www.elastic.co/guide/en/elasticsearch/reference/7.9/docs-update-by-query.html)和[reindex](https://www.elastic.co/guide/en/elasticsearch/reference/7.9/docs-reindex.html)不可用。

  \2. 高亮失效

  \3. reindex失效，原本可以修改的mapping部分参数将无法修改，并且无法升级索引

  \4. 无法查看元数据和聚合搜索

  影响索引的容灾能力

- **禁用_all字段**：_all字段的包含所有字段分词后的Term，作用是可以在搜索时不指定特定字段，从所有字段中检索，ES 6.0之前需要手动关闭

- 关闭Norms字段：计算评分用的，如果你确定当前字段将来不需要计算评分，设置false可以节省大量的磁盘空间，有助于提升性能。常见的比如filter和agg字段，都可以设为关闭。节省CPU开销

- 关闭index_options（谨慎使用，高端操作）：词设置用于在index time过程中哪些内容会被添加到倒排索引的文件中，例如TF，docCount、postion、offsets等，减少option的选项可以减少在创建索引时的CPU占用率，不过在实际场景中很难确定业务是否会用到这些信息，除非是在一开始就非常确定用不到，否则不建议删除

## 搜索速度调优

- 使用filter代替query，filter不记录相关度的评分的，query会计算相关度评分。
- 避免深度分页，避免分片数据过多，可以参考百度或者淘宝的做法。es提供两种解决方案scroll search和search after
  - scroll search： 

- 注意关于index type的使用，7.0统一了`_doc`
- 避免使用稀疏数据,
- 避免单索引业务重耦合
- 命名规范
- 冷热分离的架构设计
- fielddata：搜索时正排索引，doc_value为index time正排索引。
- enabled：是否创建倒排索引
- doc_values：正排索引，对于不需要聚合的字段，关闭正排索引可节省资源，提高查询速度

### 4、ES的节点类型（课时0.5）

- master：候选节点
- data：数据节点
- data_content：数据内容节点
- data_hot：热节点
- data_warm：索引不再定期更新，但仍可查询
- data_code：冷节点，只读索引
- Ingest：预处理节点，作用类似于Logstash中的Filter
- ml：机器学习节点
- remote_cluster_client：候选客户端节点
- transform：转换节点
- voting_only：仅投票节点

# 5. Es容灾问题

![image-20220712014350102](https://cdn.wuzx.cool/image-20220712014350102.png)

# 6.Mater选举过程



![image-20220712015152306](https://cdn.wuzx.cool/image-20220712015152306.png)

## 脑裂问题

### 可能原因

> 1.网络问题：集群间的网络延迟导致一些节点访问不到master，认为master挂掉了从而选举出新的master，并对master上的分片和副本标红，分配新的主分片
>
> 2.节点负载：主节点的角色既为master又为data，访问量较大时可能会导致ES停止响应造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。
>
> 3.内存回收：data节点上的ES进程占用的内存较大，引发JVM的大规模内存回收，造成ES进程失去响应

### 方案

> 1.减少误判：discovery.zen.ping_timeout节点状态的响应时间，默认为3s，可以适当调大，如果master在该响应时间的范围内没有做出响应应答，判断该节点已经挂掉了。调大参数（如6s，discovery.zen.ping_timeout:6），可适当减少误判。
>
> 2.选举触发 discovery.zen.minimum_master_nodes:1
>
> 该参数是用于控制选举行为发生的最小集群主节点数量。
>
> 当备选主节点的个数大于等于该参数的值，且备选主节点中有该参数个节点认为主节点挂了，进行选举。官方建议为（n/2）+1，n为主节点个数（即有资格成为主节点的节点个数）
>
> 增大该参数，当该值为2时，我们可以设置master的数量为3，这样，挂掉一台，其他两台都认为主节点挂掉了，才进行主节点选举。
>
> 3.角色分离：即master节点与data节点分离，限制角色

# 7.ElasticSearch 调优

#### 数据结构调优

> 

#### 通用法则

- 通用最小化算法：对于搜索引擎级的大数据检索，每个bit尤为珍贵。
- 业务分离：聚合和搜索分离

#### 硬件优化

es的默认配置是一个非常合理的默认配置，绝大多数情况下是不需要修改的，如果不理解某项配置的含义，没有经过验证就贸然修改默认配置，可能造成严重的后果。比如max_result_window这个设置，默认值是1W，这个设置是分页数据每页最大返回的数据量，冒然修改为较大值会导致OOM。ES没有银弹，不可能通过修改某个配置从而大幅提升ES的性能，通常出厂配置里大部分设置已经是最优配置，只有少数和具体的业务相关的设置，事先无法给出最好的默认配置，这些可能是需要我们手动去设置的。关于配置文件，如果你做不到彻底明白配置的含义，不要随意修改。

​		jvm heap分配：7.6版本默认1GB，这个值太小，很容易导致OOM。Jvm heap大小不要超过物理内存的50%，最大也不要超过32GB（compressed oop），它可用于其内部缓存的内存就越多，但可供操作系统用于文件系统缓存的内存就越少，heap过大会导致GC时间过长

- 节点：

  根据业务量不同，内存的需求也不同，一般生产建议不要少于16G。ES是比较依赖内存的，并且对内存的消耗也很大，内存对ES的重要性甚至是高于CPU的，所以即使是数据量不大的业务，为了保证服务的稳定性，在满足业务需求的前提下，我们仍需考虑留有不少于20%的冗余性能。一般来说，按照百万级、千万级、亿级数据的索引，我们为每个节点分配的内存为16G/32G/64G就足够了，太大的内存，性价比就不是那么高了。

- 内存：

  根据业务量不同，内存的需求也不同，一般生产建议不要少于16G。ES是比较依赖内存的，并且对内存的消耗也很大，内存对ES的重要性甚至是高于CPU的，所以即使是数据量不大的业务，为了保证服务的稳定性，在满足业务需求的前提下，我们仍需考虑留有不少于20%的冗余性能。一般来说，按照百万级、千万级、亿级数据的索引，我们为每个节点分配的内存为16G/32G/64G就足够了，太大的内存，性价比就不是那么高了。

- 磁盘：

  对于ES来说，磁盘可能是最重要的了，因为数据都是存储在磁盘上的，当然这里说的磁盘指的是磁盘的性能。磁盘性能往往是硬件性能的瓶颈，木桶效应中的最短板。ES应用可能要面临不间断的大量的数据读取和写入。生产环境可以考虑把节点冷热分离，“热节点”使用SSD做存储，可以大幅提高系统性能；冷数据存储在机械硬盘中，降低成本。另外，关于磁盘阵列，可以使用raid 0。

- CPU：

  CPU对计算机而言可谓是最重要的硬件，但对于ES来说，可能不是他最依赖的配置，因为提升CPU配置可能不会像提升磁盘或者内存配置带来的性能收益更直接、显著。当然也不是说CPU的性能就不重要，只不过是说，在硬件成本预算一定的前提下，应该把更多的预算花在磁盘以及内存上面。通常来说单节点cpu 4核起步，不同角色的节点对CPU的要求也不同。服务器的CPU不需要太高的单核性能，更多的核心数和线程数意味着更高的并发处理能力。现在PC的配置8核都已经普及了，更不用说服务器了。

- 网络： 

  ES是天生自带分布式属性的，并且ES的分布式系统是基于对等网络的，节点与节点之间的通信十分的频繁，延迟对于ES的用户体验是致命的，所以对于ES来说，低延迟的网络是非常有必要的。因此，使用扩地域的多个数据中心的方案是非常不可取的，ES可以容忍集群夸多个机房，可以有多个内网环境，支持跨AZ部署，但是不能接受多个机房跨地域构建集群，一旦发生了网络故障，集群可能直接GG，即使能够保证服务正常运行，维护这样（跨地域单个集群）的集群带来的额外成本可能远小于它带来的额外收益。

- 集群规划：没有最好的配置，只有最合适的配置。

- 在集群搭建之前，首先你要搞清楚，你ES cluster的使用目的是什么？主要应用于哪些场景，比如是用来存储事务日志，或者是站内搜索，或者是用于数据的聚合分析。针对不同的应用场景，应该指定不同的优化方案。

- 集群需要多少种配置（内存型/IO型/运算型），每种配置需要多少数量，通常需要和产品运营和运维测试商定，是业务量和服务器的承载能力而定，并留有一定的余量。

- 一个合理的ES集群配置应不少于5台服务器，避免脑裂时无法选举出新的Master节点的情况，另外可能还需要一些其他的单独的节点，比如ELK系统中的Kibana、Logstash等。

# es写入原理

![image-20220712164128083](https://cdn.wuzx.cool/image-20220712164128083.png)

> + node1接受请求，那么node就是协调节点，根据doc_id 计算shard的位置（shard = hash(id) % 主分片的数量）
> + 然后确定分片的位置在哪一个节点（node3），将请求转发到node3，因为分片 0 的主分片目前被分配在 Node 3 上
> + node3在主分片0执行写入操作，写入操作的内部原理
>     + `write:新增document，将document加入到内存buffer中，并且记录translog信息
>     + `refresh`:隔一段时间间隔(refresh_interval)1s refresh 生成segment(这个就是倒排索引),然后将segment放在系统缓存中，并将segment的状态设置未open，这个时候可以搜索，清空buffer
>     + `flush `:当translog信息越来越大或者每隔20分钟就会触发
>         + 清空buffer ，生成segment
>         + 调用fysnc,将缓存的segment写入磁盘
>         + 清空translogs,commit ponit被写入磁盘，标明了所有Segment
>     + 
> + 主分片写入成功之后，同步到所有的副分片，成功之后node3向协调节点报告写入成功

# es 查询原理

读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。

> + 客户端发起一个查询请求，协调节点接受请求
> + 协调将请求转发到所有的shard(可以是主分片或者副分片，采用的是随机轮询算法)
> + 每个shard 将自己的所有结果的doc_id 返回给协调节点，由协调节点进行数据的合并、排序、分页。产生最终的结果（doc id list）
> + 接着由协调节点根据doc id去各个节点拉取实际的doc 数据，返回给客户端
